{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Nonlinear Regression Solution\n",
    "#\n",
    "# This script provides a comprehensive solution for nonlinear regression,\n",
    "# combining advanced techniques with proper validation and analysis.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Data Loading and Initial Analysis\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Load data\n",
    "X = np.load(\"Data/X_train.npy\")  # shape (700, 6)\n",
    "y = np.load(\"Data/Y_train.npy\")  # shape (700,)\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Target statistics: mean={y.mean():.3f}, std={y.std():.3f}\")\n",
    "print(f\"Target range: [{y.min():.3f}, {y.max():.3f}]\")\n",
    "\n",
    "# Data visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    axes[i].scatter(X[:, i], y, alpha=0.6, s=20)\n",
    "    axes[i].set_xlabel(f'Feature {i+1}')\n",
    "    axes[i].set_ylabel('Target')\n",
    "    axes[i].set_title(f'Feature {i+1} vs Target')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Advanced Data Preprocessing\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def advanced_outlier_detection(X, y, contamination=0.05):\n",
    "    \"\"\"Advanced outlier detection combining multiple methods\"\"\"\n",
    "    \n",
    "    # Z-score method\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    z_scores = np.abs(zscore(X_scaled))\n",
    "    z_outliers = (z_scores > 3).any(axis=1)\n",
    "    \n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "    iso_outliers = iso_forest.fit_predict(X_scaled) == -1\n",
    "    \n",
    "    # Target-based outlier detection\n",
    "    y_scaled = (y - y.mean()) / y.std()\n",
    "    y_outliers = np.abs(y_scaled) > 3\n",
    "    \n",
    "    # Combine all methods\n",
    "    combined_outliers = z_outliers | iso_outliers | y_outliers\n",
    "    \n",
    "    print(f\"Outliers detected: {combined_outliers.sum()} ({combined_outliers.mean()*100:.1f}%)\")\n",
    "    print(f\"Z-score outliers: {z_outliers.sum()}\")\n",
    "    print(f\"Isolation Forest outliers: {iso_outliers.sum()}\")\n",
    "    print(f\"Target outliers: {y_outliers.sum()}\")\n",
    "    \n",
    "    return ~combined_outliers  # Return mask for clean data\n",
    "\n",
    "# Apply outlier detection\n",
    "clean_mask = advanced_outlier_detection(X, y)\n",
    "X_clean = X[clean_mask]\n",
    "y_clean = y[clean_mask]\n",
    "\n",
    "print(f\"Clean data shape: {X_clean.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.25, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Feature Engineering and Scaling\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def create_feature_engineering_pipeline():\n",
    "    \"\"\"Create comprehensive feature engineering pipeline\"\"\"\n",
    "    \n",
    "    # Multiple scaling options\n",
    "    scalers = {\n",
    "        'standard': StandardScaler(),\n",
    "        'robust': RobustScaler()\n",
    "    }\n",
    "    \n",
    "    # PCA options\n",
    "    pca_options = {\n",
    "        'no_pca': None,\n",
    "        'pca_95': PCA(n_components=0.95),\n",
    "        'pca_99': PCA(n_components=0.99)\n",
    "    }\n",
    "    \n",
    "    return scalers, pca_options\n",
    "\n",
    "scalers, pca_options = create_feature_engineering_pipeline()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Advanced RBF Implementation\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def advanced_rbf_transform(X, centers, sigma, method='gaussian'):\n",
    "    \"\"\"Advanced RBF transformation with multiple kernel types\"\"\"\n",
    "    \n",
    "    diff = X[:, np.newaxis, :] - centers[np.newaxis, :, :]\n",
    "    \n",
    "    if method == 'gaussian':\n",
    "        return np.exp(-np.sum(diff**2, axis=2) / (2*sigma**2))\n",
    "    elif method == 'multiquadric':\n",
    "        return np.sqrt(np.sum(diff**2, axis=2) + sigma**2)\n",
    "    elif method == 'inverse_multiquadric':\n",
    "        return 1.0 / np.sqrt(np.sum(diff**2, axis=2) + sigma**2)\n",
    "    elif method == 'thin_plate_spline':\n",
    "        r = np.sqrt(np.sum(diff**2, axis=2))\n",
    "        return r**2 * np.log(r + 1e-10)  # avoid log(0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown RBF method: {method}\")\n",
    "\n",
    "def optimize_rbf_centers(X, n_centers, method='kmeans'):\n",
    "    \"\"\"Optimize RBF center selection\"\"\"\n",
    "    \n",
    "    if method == 'kmeans':\n",
    "        kmeans = KMeans(n_clusters=n_centers, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        return kmeans.cluster_centers_\n",
    "    elif method == 'random':\n",
    "        np.random.seed(42)\n",
    "        return X[np.random.choice(X.shape[0], n_centers, replace=False)]\n",
    "    elif method == 'uniform':\n",
    "        centers = np.zeros((n_centers, X.shape[1]))\n",
    "        for i in range(X.shape[1]):\n",
    "            centers[:, i] = np.linspace(X[:, i].min(), X[:, i].max(), n_centers)\n",
    "        return centers\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown center method: {method}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5. Comprehensive Model Suite\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def get_model_suite():\n",
    "    \"\"\"Get comprehensive suite of nonlinear regression models\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        # Kernel methods\n",
    "        'KernelRidge_RBF': KernelRidge(kernel='rbf'),\n",
    "        'KernelRidge_Poly': KernelRidge(kernel='polynomial'),\n",
    "        'KernelRidge_Linear': KernelRidge(kernel='linear'),\n",
    "        'SVR_RBF': SVR(kernel='rbf'),\n",
    "        'SVR_Poly': SVR(kernel='poly'),\n",
    "        \n",
    "        # Tree-based methods\n",
    "        'RandomForest': RandomForestRegressor(random_state=42),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "        \n",
    "        # Neural networks\n",
    "        'MLP_Default': MLPRegressor(random_state=42, max_iter=1000),\n",
    "        'MLP_Large': MLPRegressor(hidden_layer_sizes=(100, 50), random_state=42, max_iter=1000),\n",
    "        \n",
    "        # Linear with regularization\n",
    "        'Ridge': Ridge(),\n",
    "        'ElasticNet': ElasticNet(random_state=42)\n",
    "    }\n",
    "    \n",
    "    return models\n",
    "\n",
    "def get_hyperparameter_grids():\n",
    "    \"\"\"Get hyperparameter grids for each model\"\"\"\n",
    "    \n",
    "    param_grids = {\n",
    "        'KernelRidge_RBF': {'alpha': [0.001, 0.01, 0.1, 1.0], 'gamma': [0.01, 0.1, 1.0, 10.0]},\n",
    "        'KernelRidge_Poly': {'alpha': [0.001, 0.01, 0.1, 1.0], 'degree': [2, 3, 4, 5]},\n",
    "        'SVR_RBF': {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1.0], 'epsilon': [0.01, 0.1, 0.2]},\n",
    "        'RandomForest': {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, None], 'min_samples_split': [2, 5, 10]},\n",
    "        'GradientBoosting': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]},\n",
    "        'Ridge': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]},\n",
    "        'ElasticNet': {'alpha': [0.001, 0.01, 0.1, 1.0], 'l1_ratio': [0.1, 0.5, 0.7, 0.9]}\n",
    "    }\n",
    "    \n",
    "    return param_grids\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6. Advanced RBF with Multiple Kernels\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def optimize_rbf_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Optimize RBF model with multiple kernels and center selection methods\"\"\"\n",
    "    \n",
    "    sigma_list = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "    n_centers_list = [50, 100, 150, 200, 250]\n",
    "    alpha_list = [0.001, 0.01, 0.1, 1.0]\n",
    "    rbf_methods = ['gaussian', 'multiquadric', 'inverse_multiquadric']\n",
    "    center_methods = ['kmeans', 'random', 'uniform']\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    results = []\n",
    "    \n",
    "    print(\"Optimizing RBF model...\")\n",
    "    \n",
    "    for sigma, n_centers, alpha, rbf_method, center_method in product(\n",
    "        sigma_list, n_centers_list, alpha_list, rbf_methods, center_methods\n",
    "    ):\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_train):\n",
    "            X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "            \n",
    "            try:\n",
    "                centers = optimize_rbf_centers(X_tr, n_centers, center_method)\n",
    "                X_tr_rbf = advanced_rbf_transform(X_tr, centers, sigma, rbf_method)\n",
    "                X_val_rbf = advanced_rbf_transform(X_val, centers, sigma, rbf_method)\n",
    "                \n",
    "                model = Ridge(alpha=alpha)\n",
    "                model.fit(X_tr_rbf, y_tr)\n",
    "                \n",
    "                y_val_pred = model.predict(X_val_rbf)\n",
    "                scores.append(r2_score(y_val, y_val_pred))\n",
    "            except:\n",
    "                scores.append(-np.inf)\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        results.append({\n",
    "            'sigma': sigma, 'n_centers': n_centers, 'alpha': alpha,\n",
    "            'rbf_method': rbf_method, 'center_method': center_method,\n",
    "            'score': mean_score\n",
    "        })\n",
    "        \n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = {\n",
    "                'sigma': sigma, 'n_centers': n_centers, 'alpha': alpha,\n",
    "                'rbf_method': rbf_method, 'center_method': center_method\n",
    "            }\n",
    "    \n",
    "    print(\"Best RBF model score:\", best_score)\n",
    "    print(\"Best RBF model parameters:\", best_params)\n",
    "    return best_params, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c950fdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train_input = np.load(\"Data/X_train.npy\")  # shape (700, 6)\n",
    "Y_train_input = np.load(\"Data/Y_train.npy\")  # shape (700,)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_train_input, Y_train_input, test_size=200, random_state=19, shuffle=True\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e75c8ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99)  # keep 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVR R²: 0.4873093147885711 Test RMSE: 1.0126070765879374\n",
      "SVR best params: {'C': 0.1, 'epsilon': 0.5, 'gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "#  SVR with RBF kernel (GridSearchCV)\n",
    "# ===================================\n",
    "svr_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'epsilon': [0.01, 0.1, 0.5],\n",
    "    'gamma': ['1']\n",
    "}\n",
    "\n",
    "svr = SVR(kernel='poly')\n",
    "svr_grid = GridSearchCV(svr, svr_params, cv=5, scoring='r2')\n",
    "svr_grid.fit(X_train_pca, Y_train)\n",
    "svr_best = svr_grid.best_estimator_\n",
    "svr_pred = svr_best.predict(X_test_pca)\n",
    "print(\"Best SVR R²:\", svr_grid.best_score_, \"Test RMSE:\", np.sqrt(mean_squared_error(Y_test, svr_pred)))\n",
    "print(\"SVR best params:\", svr_grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
