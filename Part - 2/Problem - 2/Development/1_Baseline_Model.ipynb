{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86d4cff",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c3ebc544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7addd095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(444, 3)\n",
      "(14,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train = joblib.load(\"Data/Xtrain2.pkl\")\n",
    "Y_train = np.load('Data/Ytrain2.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1fb86e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERING OPTIONS - MODIFY THESE TO REMOVE EXERCISES OR PATIENTS\n",
    "\n",
    "# Option 1: Remove specific exercises (comment out if you want all exercises)\n",
    "# exercises_to_remove = ['E1', 'E2']  # Remove exercises E1 and E2\n",
    "exercises_to_remove = ['E2']  # Keep all exercises\n",
    "\n",
    "# Option 2: Remove specific patients (comment out if you want all patients)\n",
    "# patients_to_remove = [1, 5, 10]  # Remove patients 1, 5, and 10\n",
    "patients_to_remove = []  # Keep all patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9316c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed exercises ['E2']. Sequences: 444 -> 390\n",
      "Patient IDs in filtered data: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "# Apply filters if specified\n",
    "if exercises_to_remove:\n",
    "    original_count = len(X_train)\n",
    "    X_train = X_train[~X_train['Exercise_Id'].isin(exercises_to_remove)]\n",
    "    print(f\"Removed exercises {exercises_to_remove}. Sequences: {original_count} -> {len(X_train)}\")\n",
    "\n",
    "if patients_to_remove:\n",
    "    original_count = len(X_train)\n",
    "    X_train = X_train[~X_train['Patient_Id'].isin(patients_to_remove)]\n",
    "    print(f\"Removed patients {patients_to_remove}. Sequences: {original_count} -> {len(X_train)}\")\n",
    "\n",
    "# Get patient IDs from the filtered data\n",
    "patient_ids = np.sort(X_train['Patient_Id'].unique())\n",
    "print(f\"Patient IDs in filtered data: {patient_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fef5c6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data: 390 sequences, 14 patients\n",
      "Class distribution: 9 left vs 5 right impaired\n"
     ]
    }
   ],
   "source": [
    "# Create patient-to-label mapping from the original Y_train\n",
    "# We need to filter Y_train to only include patients that remain after filtering\n",
    "all_patient_to_label = dict(zip(range(1, 15), Y_train))  # Original mapping for patients 1-14\n",
    "patient_to_label = {pid: all_patient_to_label[pid] for pid in patient_ids}\n",
    "Y_train_filtered = np.array([patient_to_label[pid] for pid in patient_ids])\n",
    "\n",
    "print(f\"Filtered data: {X_train.shape[0]} sequences, {len(patient_ids)} patients\")\n",
    "print(f\"Class distribution: {np.sum(Y_train_filtered==0)} left vs {np.sum(Y_train_filtered==1)} right impaired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "edc4e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction function\n",
    "def extract_features(df):\n",
    "    features_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        skeleton_seq = row['Skeleton_Sequence']\n",
    "        # skeleton_seq shape: (seq_length, 33, 2)\n",
    "        \n",
    "        # Flatten to (seq_length, 66)\n",
    "        flattened = skeleton_seq.reshape(len(skeleton_seq), -1)\n",
    "        \n",
    "        # Mean and variance for each of 66 coordinates\n",
    "        means = np.mean(flattened, axis=0)\n",
    "        variances = np.var(flattened, axis=0)\n",
    "        \n",
    "        features_list.append(np.concatenate([means, variances]))\n",
    "    \n",
    "    return np.array(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3ea39bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all sequences\n",
    "X_all_features = extract_features(X_train)\n",
    "\n",
    "# Prepare one-hot encoder for exercises\n",
    "exercise_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "exercise_encoded = exercise_encoder.fit_transform(X_train[['Exercise_Id']])\n",
    "\n",
    "# Combine features with exercise encoding\n",
    "X_combined = np.concatenate([X_all_features, exercise_encoded], axis=1)\n",
    "\n",
    "# Create sequence-level labels\n",
    "patient_to_label = dict(zip(patient_ids, Y_train))\n",
    "y_sequences = X_train['Patient_Id'].map(patient_to_label).values\n",
    "\n",
    "# Store patient ID for each sequence\n",
    "sequence_patient_ids = X_train['Patient_Id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ca116ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.19733912e-03, -5.65041175e-01,  1.50232159e-02, -5.90743209e-01,\n",
       "        1.83890064e-02, -5.90576831e-01,  1.58385386e-02, -5.90982177e-01,\n",
       "       -7.57782500e-03, -5.93582941e-01, -3.81967188e-03, -5.92943849e-01,\n",
       "        9.05467424e-04, -5.84129801e-01,  7.47027613e-02, -5.76397716e-01,\n",
       "       -5.48540568e-02, -5.25465254e-01,  4.08378212e-02, -5.57124736e-01,\n",
       "        8.62539943e-03, -5.19998277e-01,  1.92334260e-01, -4.19843892e-01,\n",
       "       -1.35340785e-01, -4.79021439e-01,  2.42496352e-01, -2.16031929e-01,\n",
       "       -2.21562543e-01, -2.78345187e-01,  2.10580915e-01, -3.52807414e-02,\n",
       "       -1.08712225e-01, -2.85554500e-01,  1.89105874e-01,  8.73673867e-03,\n",
       "       -5.02839747e-02, -2.82023093e-01,  1.47100941e-01,  6.96730483e-03,\n",
       "       -9.50986984e-03, -3.38909182e-01,  1.92108549e-01, -7.51216143e-03,\n",
       "       -8.43244232e-02, -3.01323159e-01,  1.01910927e-01, -3.83950593e-03,\n",
       "       -1.00125355e-01,  1.69220185e-03,  2.76740440e-01,  1.84318775e-01,\n",
       "       -1.39805395e-01,  1.30923452e-01,  2.26545868e-01,  5.76607316e-01,\n",
       "       -1.61463956e-01,  5.55260296e-01,  2.01378188e-01,  6.13210764e-01,\n",
       "       -1.66025237e-01,  5.91753036e-01,  2.58176125e-01,  6.48218262e-01,\n",
       "       -2.26300585e-01,  6.07441400e-01,  3.74318933e-04,  4.92235865e-04,\n",
       "        3.63174734e-04,  4.09425851e-04,  3.46028374e-04,  4.27738649e-04,\n",
       "        3.41919676e-04,  4.27074743e-04,  3.46805439e-04,  4.41047270e-04,\n",
       "        3.54365651e-04,  4.50551836e-04,  3.42828051e-04,  4.40190899e-04,\n",
       "        1.40232685e-04,  2.30956325e-04,  1.85350553e-04,  2.39833497e-04,\n",
       "        2.64592064e-04,  3.92193583e-04,  2.42131941e-04,  3.99712630e-04,\n",
       "        3.29176411e-05,  1.05752742e-04,  1.89751354e-04,  3.21900519e-04,\n",
       "        1.45516168e-04,  5.82457050e-05,  1.04798104e-03,  5.19234868e-03,\n",
       "        1.00229066e-04,  3.67500398e-05,  6.36231806e-03,  3.53580288e-02,\n",
       "        1.76775769e-04,  6.17958249e-05,  7.72382849e-03,  4.58613796e-02,\n",
       "        2.70378551e-04,  1.86730245e-04,  6.77884072e-03,  4.79409384e-02,\n",
       "        1.27480148e-04,  6.86832265e-05,  5.91288786e-03,  3.79099087e-02,\n",
       "        2.58785491e-06,  1.22322611e-05,  2.19318532e-06,  1.09742461e-05,\n",
       "        4.84429936e-05,  7.51283469e-05,  7.64637362e-05,  2.56422161e-04,\n",
       "        2.66913154e-05,  8.03855461e-05,  3.00377265e-05,  6.62115110e-05,\n",
       "        3.14588148e-05,  8.09463519e-05,  3.37924497e-05,  6.32787465e-05,\n",
       "        5.96966502e-05,  2.38807366e-04,  4.08310885e-05,  1.32374914e-04,\n",
       "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a6da1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "n_splits = 1000\n",
    "majority_accuracies = []\n",
    "probability_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2a995da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in range(n_splits):\n",
    "    # Random stratified split: 2 left + 1 right patients for test\n",
    "    rng = np.random.RandomState(42 + split)\n",
    "    \n",
    "    left_patients = patient_ids[Y_train == 0]  # Patients with label 0\n",
    "    right_patients = patient_ids[Y_train == 1]  # Patients with label 1\n",
    "    \n",
    "    test_left = rng.choice(left_patients, size=2, replace=False)\n",
    "    test_right = rng.choice(right_patients, size=1, replace=False)\n",
    "    test_patients = np.concatenate([test_left, test_right])\n",
    "    train_patients = np.setdiff1d(patient_ids, test_patients)\n",
    "    \n",
    "    # Create masks for sequences (much faster than filtering DataFrame)\n",
    "    train_mask = np.isin(sequence_patient_ids, train_patients)\n",
    "    test_mask = np.isin(sequence_patient_ids, test_patients)\n",
    "    \n",
    "    # Split the precomputed features\n",
    "    X_train_split = X_combined[train_mask]\n",
    "    X_test_split = X_combined[test_mask]\n",
    "    y_train_split = y_sequences[train_mask]\n",
    "    y_test_split = y_sequences[test_mask]\n",
    "    test_patient_ids_split = sequence_patient_ids[test_mask]\n",
    "     \n",
    "    # Train SVM\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # Get both predictions and probabilities for test sequences\n",
    "    test_sequence_preds = model.predict(X_test_split)\n",
    "    test_sequence_probs = model.predict_proba(X_test_split)\n",
    "    \n",
    "    # METHOD 1: Majority Voting\n",
    "    patient_predictions = {}\n",
    "    for i, patient_id in enumerate(test_patient_ids_split):\n",
    "        if patient_id not in patient_predictions:\n",
    "            patient_predictions[patient_id] = []\n",
    "        patient_predictions[patient_id].append(test_sequence_preds[i])\n",
    "    \n",
    "    majority_preds = []\n",
    "    true_labels = []\n",
    "    for patient_id in test_patients:\n",
    "        votes = patient_predictions[patient_id]\n",
    "        pred_label = np.argmax(np.bincount(votes))\n",
    "        majority_preds.append(pred_label)\n",
    "        true_labels.append(patient_to_label[patient_id])\n",
    "    \n",
    "    majority_acc = balanced_accuracy_score(true_labels, majority_preds)\n",
    "    majority_accuracies.append(majority_acc)\n",
    "    \n",
    "    # METHOD 2: Probability Averaging\n",
    "    patient_probs = {}\n",
    "    for i, patient_id in enumerate(test_patient_ids_split):\n",
    "        if patient_id not in patient_probs:\n",
    "            patient_probs[patient_id] = []\n",
    "        patient_probs[patient_id].append(test_sequence_probs[i])\n",
    "    \n",
    "    probability_preds = []\n",
    "    for patient_id in test_patients:\n",
    "        avg_probs = np.mean(patient_probs[patient_id], axis=0)\n",
    "        pred_label = np.argmax(avg_probs)\n",
    "        probability_preds.append(pred_label)\n",
    "    \n",
    "    probability_acc = balanced_accuracy_score(true_labels, probability_preds)\n",
    "    probability_accuracies.append(probability_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "360628ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARISON RESULTS ===\n",
      "\n",
      "MAJORITY VOTING:\n",
      "  Mean Balanced Accuracy: 0.4032 (±0.2252)\n",
      "  Min: 0.0000, Max: 1.0000\n",
      "\n",
      "PROBABILITY AVERAGING:\n",
      "  Mean Balanced Accuracy: 0.4135 (±0.1975)\n",
      "  Min: 0.0000, Max: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Final results comparison\n",
    "print(f\"\\n=== COMPARISON RESULTS ===\\n\")\n",
    "print(f\"MAJORITY VOTING:\")\n",
    "print(f\"  Mean Balanced Accuracy: {np.mean(majority_accuracies):.4f} (±{np.std(majority_accuracies):.4f})\")\n",
    "print(f\"  Min: {np.min(majority_accuracies):.4f}, Max: {np.max(majority_accuracies):.4f}\")\n",
    "\n",
    "print(f\"\\nPROBABILITY AVERAGING:\")\n",
    "print(f\"  Mean Balanced Accuracy: {np.mean(probability_accuracies):.4f} (±{np.std(probability_accuracies):.4f})\")\n",
    "print(f\"  Min: {np.min(probability_accuracies):.4f}, Max: {np.max(probability_accuracies):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
