{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42827553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9aac7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded feature importance data: 775 rows\n"
     ]
    }
   ],
   "source": [
    "# Load feature importance data\n",
    "feature_importance_df = pd.read_csv('feature_importance_v2_baseline.csv')\n",
    "print(f\"Loaded feature importance data: {len(feature_importance_df)} rows\")\n",
    "\n",
    "# Create feature name to index mapping\n",
    "keypoint_names = [\n",
    "    'nose', 'left_eye_inner', 'left_eye', 'left_eye_outer', 'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "    'left_ear', 'right_ear', 'mouth_left', 'mouth_right', 'left_shoulder', 'right_shoulder',\n",
    "    'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_pinky', 'right_pinky',\n",
    "    'left_index', 'right_index', 'left_thumb', 'right_thumb', 'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle', 'left_heel', 'right_heel',\n",
    "    'left_foot_index', 'right_foot_index'\n",
    "]\n",
    "\n",
    "feature_to_index = {}\n",
    "for i, name in enumerate(keypoint_names):\n",
    "    feature_to_index[f'{name}_mean_x'] = (i, 'mean_x')\n",
    "    feature_to_index[f'{name}_mean_y'] = (i, 'mean_y')\n",
    "    feature_to_index[f'{name}_var_x'] = (i, 'var_x')\n",
    "    feature_to_index[f'{name}_var_y'] = (i, 'var_y')\n",
    "\n",
    "# Dynamic feature names and their indices (they come after the 132 static features)\n",
    "dynamic_feature_names = [\n",
    "    'left_side_mean_velocity', 'left_side_std_velocity', 'left_side_max_velocity',\n",
    "    'right_side_mean_velocity', 'right_side_std_velocity', 'right_side_max_velocity',\n",
    "    'left_side_mean_acceleration', 'left_side_std_acceleration', 'left_side_max_acceleration',\n",
    "    'right_side_mean_acceleration', 'right_side_std_acceleration', 'right_side_max_acceleration',\n",
    "    'velocity_mean_asymmetry_ratio', 'velocity_std_asymmetry_ratio', 'velocity_max_asymmetry_ratio',\n",
    "    'acceleration_mean_asymmetry_ratio', 'acceleration_std_asymmetry_ratio', 'acceleration_max_asymmetry_ratio'\n",
    "]\n",
    "\n",
    "# Add dynamic features to the mapping (they start at index 132)\n",
    "for i, name in enumerate(dynamic_feature_names):\n",
    "    feature_to_index[name] = (132 + i, 'dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e5ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define left and right side keypoints\n",
    "LEFT_SIDE_KEYPOINTS = [11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]  # Left body parts\n",
    "RIGHT_SIDE_KEYPOINTS = [12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]  # Right body parts\n",
    "\n",
    "def calculate_velocity(sequence):\n",
    "    \"\"\"Calculate velocity between consecutive frames\"\"\"\n",
    "    # sequence shape: (seq_length, 33, 2)\n",
    "    velocity = np.diff(sequence, axis=0)  # (seq_length-1, 33, 2)\n",
    "    return velocity\n",
    "\n",
    "def calculate_side_velocity_stats(velocity_sequence, side_keypoints):\n",
    "    \"\"\"Calculate velocity statistics for a body side\"\"\"\n",
    "    # velocity_sequence shape: (seq_length-1, 33, 2)\n",
    "    side_velocity = velocity_sequence[:, side_keypoints, :]  # (seq_length-1, n_keypoints, 2)\n",
    "    \n",
    "    # Calculate magnitude of velocity vectors\n",
    "    velocity_magnitudes = np.sqrt(np.sum(side_velocity**2, axis=2))  # (seq_length-1, n_keypoints)\n",
    "    \n",
    "    # Aggregate statistics\n",
    "    mean_velocity = np.mean(velocity_magnitudes)\n",
    "    std_velocity = np.std(velocity_magnitudes)\n",
    "    max_velocity = np.max(velocity_magnitudes)\n",
    "    \n",
    "    return mean_velocity, std_velocity, max_velocity\n",
    "\n",
    "def calculate_acceleration(velocity_sequence):\n",
    "    \"\"\"Calculate acceleration from velocity sequence\"\"\"\n",
    "    # velocity_sequence shape: (seq_length-1, 33, 2)\n",
    "    acceleration = np.diff(velocity_sequence, axis=0)  # (seq_length-2, 33, 2)\n",
    "    return acceleration\n",
    "\n",
    "def calculate_side_acceleration_stats(acceleration_sequence, side_keypoints):\n",
    "    \"\"\"Calculate acceleration statistics for a body side\"\"\"\n",
    "    # acceleration_sequence shape: (seq_length-2, 33, 2)\n",
    "    side_acceleration = acceleration_sequence[:, side_keypoints, :]  # (seq_length-2, n_keypoints, 2)\n",
    "    \n",
    "    # Calculate magnitude of acceleration vectors\n",
    "    acceleration_magnitudes = np.sqrt(np.sum(side_acceleration**2, axis=2))  # (seq_length-2, n_keypoints)\n",
    "    \n",
    "    # Aggregate statistics\n",
    "    mean_acceleration = np.mean(acceleration_magnitudes)\n",
    "    std_acceleration = np.std(acceleration_magnitudes)\n",
    "    max_acceleration = np.max(acceleration_magnitudes)\n",
    "    \n",
    "    return mean_acceleration, std_acceleration, max_acceleration\n",
    "\n",
    "def calculate_asymmetry_features(left_stats, right_stats):\n",
    "    \"\"\"Calculate asymmetry ratios between left and right sides\"\"\"\n",
    "    left_mean_vel, left_std_vel, left_max_vel = left_stats['velocity']\n",
    "    right_mean_vel, right_std_vel, right_max_vel = right_stats['velocity']\n",
    "    \n",
    "    left_mean_acc, left_std_acc, left_max_acc = left_stats['acceleration']\n",
    "    right_mean_acc, right_std_acc, right_max_acc = right_stats['acceleration']\n",
    "    \n",
    "    # Velocity asymmetry ratios\n",
    "    vel_mean_ratio = left_mean_vel / (right_mean_vel + 1e-8)  # Avoid division by zero\n",
    "    vel_std_ratio = left_std_vel / (right_std_vel + 1e-8)\n",
    "    vel_max_ratio = left_max_vel / (right_max_vel + 1e-8)\n",
    "    \n",
    "    # Acceleration asymmetry ratios\n",
    "    acc_mean_ratio = left_mean_acc / (right_mean_acc + 1e-8)\n",
    "    acc_std_ratio = left_std_acc / (right_std_acc + 1e-8)\n",
    "    acc_max_ratio = left_max_acc / (right_max_acc + 1e-8)\n",
    "    \n",
    "    return [vel_mean_ratio, vel_std_ratio, vel_max_ratio, \n",
    "            acc_mean_ratio, acc_std_ratio, acc_max_ratio]\n",
    "\n",
    "def extract_dynamic_features(df):\n",
    "    dynamic_features_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        skeleton_seq = np.array(row['Skeleton_Sequence'])\n",
    "\n",
    "\n",
    "        skeleton_seq = skeleton_seq.reshape(skeleton_seq.shape[0], 33, 2)\n",
    "\n",
    "        # Calculate velocity and acceleration\n",
    "        velocity = calculate_velocity(skeleton_seq)\n",
    "        acceleration = calculate_acceleration(velocity)\n",
    "        \n",
    "        # Calculate statistics for each side\n",
    "        left_vel_stats = calculate_side_velocity_stats(velocity, LEFT_SIDE_KEYPOINTS)\n",
    "        right_vel_stats = calculate_side_velocity_stats(velocity, RIGHT_SIDE_KEYPOINTS)\n",
    "        \n",
    "        left_acc_stats = calculate_side_acceleration_stats(acceleration, LEFT_SIDE_KEYPOINTS)\n",
    "        right_acc_stats = calculate_side_acceleration_stats(acceleration, RIGHT_SIDE_KEYPOINTS)\n",
    "        \n",
    "        # Create feature dictionaries\n",
    "        left_stats = {'velocity': left_vel_stats, 'acceleration': left_acc_stats}\n",
    "        right_stats = {'velocity': right_vel_stats, 'acceleration': right_acc_stats}\n",
    "        \n",
    "        # Calculate asymmetry features\n",
    "        asymmetry_features = calculate_asymmetry_features(left_stats, right_stats)\n",
    "        \n",
    "        # Combine all dynamic features\n",
    "        dynamic_features = [\n",
    "            *left_vel_stats, *right_vel_stats,\n",
    "            *left_acc_stats, *right_acc_stats,\n",
    "            *asymmetry_features\n",
    "        ]\n",
    "        \n",
    "        dynamic_features_list.append(dynamic_features)\n",
    "    \n",
    "    return np.array(dynamic_features_list)\n",
    "\n",
    "# Helper function to extract dynamic features for a single sequence\n",
    "def extract_dynamic_features_single(skeleton_seq):\n",
    "    \"\"\"Extract dynamic features for a single skeleton sequence\"\"\"\n",
    "    # Calculate velocity and acceleration\n",
    "    velocity = calculate_velocity(skeleton_seq)\n",
    "    acceleration = calculate_acceleration(velocity)\n",
    "    \n",
    "    # Calculate statistics for each side\n",
    "    left_vel_stats = calculate_side_velocity_stats(velocity, LEFT_SIDE_KEYPOINTS)\n",
    "    right_vel_stats = calculate_side_velocity_stats(velocity, RIGHT_SIDE_KEYPOINTS)\n",
    "    \n",
    "    left_acc_stats = calculate_side_acceleration_stats(acceleration, LEFT_SIDE_KEYPOINTS)\n",
    "    right_acc_stats = calculate_side_acceleration_stats(acceleration, RIGHT_SIDE_KEYPOINTS)\n",
    "    \n",
    "    # Create feature dictionaries\n",
    "    left_stats = {\n",
    "        'velocity': left_vel_stats,\n",
    "        'acceleration': left_acc_stats\n",
    "    }\n",
    "    right_stats = {\n",
    "        'velocity': right_vel_stats,\n",
    "        'acceleration': right_acc_stats\n",
    "    }\n",
    "    \n",
    "    # Calculate asymmetry features\n",
    "    asymmetry_features = calculate_asymmetry_features(left_stats, right_stats)\n",
    "    \n",
    "    # Combine all dynamic features\n",
    "    dynamic_features = [\n",
    "        *left_vel_stats, *right_vel_stats,    # 6 velocity features\n",
    "        *left_acc_stats, *right_acc_stats,    # 6 acceleration features  \n",
    "        *asymmetry_features                   # 6 asymmetry ratios\n",
    "    ]\n",
    "    \n",
    "    return np.array(dynamic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa5e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined feature extraction function with top-N masking for BOTH static and dynamic features\n",
    "def extract_combined_features_with_masking(df, top_n_mask):\n",
    "    features_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        skeleton_seq = row['Skeleton_Sequence']\n",
    "        exercise_id = row['Exercise_Id']\n",
    "        \n",
    "        # Get feature mask configuration for this exercise\n",
    "        feature_mask_config = top_n_mask.get(exercise_id, {})\n",
    "        \n",
    "        # ===== EXTRACT STATIC FEATURES WITH MASKING =====\n",
    "        # Ensure skeleton_seq is 3D: (seq_length, 33, 2)\n",
    "        if skeleton_seq.ndim == 2 and skeleton_seq.shape[1] == 66:\n",
    "            skeleton_seq = skeleton_seq.reshape(skeleton_seq.shape[0], 33, 2)\n",
    "        \n",
    "        # Calculate means and variances for all keypoints first\n",
    "        flattened = skeleton_seq.reshape(len(skeleton_seq), -1)  # (seq_length, 66)\n",
    "        all_means = np.mean(flattened, axis=0)  # 66 features\n",
    "        all_variances = np.var(flattened, axis=0)  # 66 features\n",
    "        \n",
    "        # Apply granular masking - only keep specified static features\n",
    "        final_means = np.zeros(66)\n",
    "        final_variances = np.zeros(66)\n",
    "        \n",
    "        # For each keypoint in the mask configuration, keep specified components\n",
    "        for kp_idx, components_to_keep in feature_mask_config.items():\n",
    "            if kp_idx == 'dynamic':\n",
    "                continue  # Skip dynamic features for now\n",
    "            \n",
    "            # Each keypoint has 2 positions in the mean/variance arrays\n",
    "            mean_x_idx = kp_idx * 2\n",
    "            mean_y_idx = kp_idx * 2 + 1\n",
    "            var_x_idx = kp_idx * 2\n",
    "            var_y_idx = kp_idx * 2 + 1\n",
    "            \n",
    "            if 'mean_x' in components_to_keep:\n",
    "                final_means[mean_x_idx] = all_means[mean_x_idx]\n",
    "            if 'mean_y' in components_to_keep:\n",
    "                final_means[mean_y_idx] = all_means[mean_y_idx]\n",
    "            if 'var_x' in components_to_keep:\n",
    "                final_variances[var_x_idx] = all_variances[var_x_idx]\n",
    "            if 'var_y' in components_to_keep:\n",
    "                final_variances[var_y_idx] = all_variances[var_y_idx]\n",
    "        \n",
    "        static_features = np.concatenate([final_means, final_variances])\n",
    "        \n",
    "        # ===== EXTRACT DYNAMIC FEATURES WITH MASKING =====\n",
    "        dynamic_features_all = extract_dynamic_features_single(skeleton_seq)\n",
    "        \n",
    "        # Apply masking to dynamic features\n",
    "        dynamic_features_masked = np.zeros(len(dynamic_feature_names))\n",
    "        if 'dynamic' in feature_mask_config:\n",
    "            dynamic_indices_to_keep = feature_mask_config['dynamic']\n",
    "            for idx in dynamic_indices_to_keep:\n",
    "                if idx < len(dynamic_features_all):\n",
    "                    dynamic_features_masked[idx] = dynamic_features_all[idx]\n",
    "        \n",
    "        # ===== COMBINE STATIC AND DYNAMIC FEATURES =====\n",
    "        combined_features = np.concatenate([static_features, dynamic_features_masked])\n",
    "        features_list.append(combined_features)\n",
    "    \n",
    "    return np.array(features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "777a92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create feature mask for top N features per exercise (BOTH static and dynamic)\n",
    "def create_top_n_mask(n_features_per_exercise):\n",
    "    mask_dict = {}\n",
    "    \n",
    "    for exercise in ['E1', 'E2', 'E3', 'E4', 'E5']:\n",
    "        # Get top N features for this exercise (both static and dynamic)\n",
    "        top_features = feature_importance_df[\n",
    "            feature_importance_df['exercise'] == exercise\n",
    "        ].nlargest(n_features_per_exercise, 'importance')\n",
    "        \n",
    "        mask_dict[exercise] = {}\n",
    "        \n",
    "        for _, row in top_features.iterrows():\n",
    "            feature_name = row['feature']\n",
    "            if feature_name in feature_to_index:\n",
    "                feature_idx, feature_type = feature_to_index[feature_name]\n",
    "                \n",
    "                if feature_type == 'dynamic':\n",
    "                    # For dynamic features, we store them with a special key\n",
    "                    if 'dynamic' not in mask_dict[exercise]:\n",
    "                        mask_dict[exercise]['dynamic'] = set()\n",
    "                    mask_dict[exercise]['dynamic'].add(feature_idx - 132)  # Convert to dynamic feature index (0-17)\n",
    "                else:\n",
    "                    # For static features, use the original keypoint-based system\n",
    "                    kp_idx = feature_idx\n",
    "                    component = feature_type\n",
    "                    if kp_idx not in mask_dict[exercise]:\n",
    "                        mask_dict[exercise][kp_idx] = []\n",
    "                    if component not in mask_dict[exercise][kp_idx]:\n",
    "                        mask_dict[exercise][kp_idx].append(component)\n",
    "    \n",
    "    return mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37e301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature name to index mapping\n",
    "keypoint_names = [\n",
    "    'nose', 'left_eye_inner', 'left_eye', 'left_eye_outer', 'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "    'left_ear', 'right_ear', 'mouth_left', 'mouth_right', 'left_shoulder', 'right_shoulder',\n",
    "    'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_pinky', 'right_pinky',\n",
    "    'left_index', 'right_index', 'left_thumb', 'right_thumb', 'left_hip', 'right_hip',\n",
    "    'left_knee', 'right_knee', 'left_ankle', 'right_ankle', 'left_heel', 'right_heel',\n",
    "    'left_foot_index', 'right_foot_index'\n",
    "]\n",
    "\n",
    "feature_to_index = {}\n",
    "for i, name in enumerate(keypoint_names):\n",
    "    feature_to_index[f'{name}_mean_x'] = (i, 'mean_x')\n",
    "    feature_to_index[f'{name}_mean_y'] = (i, 'mean_y')\n",
    "    feature_to_index[f'{name}_var_x'] = (i, 'var_x')\n",
    "    feature_to_index[f'{name}_var_y'] = (i, 'var_y')\n",
    "\n",
    "# Dynamic feature names and their indices (they come after the 132 static features)\n",
    "dynamic_feature_names = [\n",
    "    'left_side_mean_velocity', 'left_side_std_velocity', 'left_side_max_velocity',\n",
    "    'right_side_mean_velocity', 'right_side_std_velocity', 'right_side_max_velocity',\n",
    "    'left_side_mean_acceleration', 'left_side_std_acceleration', 'left_side_max_acceleration',\n",
    "    'right_side_mean_acceleration', 'right_side_std_acceleration', 'right_side_max_acceleration',\n",
    "    'velocity_mean_asymmetry_ratio', 'velocity_std_asymmetry_ratio', 'velocity_max_asymmetry_ratio',\n",
    "    'acceleration_mean_asymmetry_ratio', 'acceleration_std_asymmetry_ratio', 'acceleration_max_asymmetry_ratio'\n",
    "]\n",
    "\n",
    "# Add dynamic features to the mapping (they start at index 132)\n",
    "for i, name in enumerate(dynamic_feature_names):\n",
    "    feature_to_index[name] = (132 + i, 'dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ab6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define left and right side keypoints\n",
    "LEFT_SIDE_KEYPOINTS = [11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]  # Left body parts\n",
    "RIGHT_SIDE_KEYPOINTS = [12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]  # Right body parts\n",
    "\n",
    "def calculate_velocity(sequence):\n",
    "    \"\"\"Calculate velocity between consecutive frames\"\"\"\n",
    "    # sequence shape: (seq_length, 33, 2)\n",
    "    velocity = np.diff(sequence, axis=0)  # (seq_length-1, 33, 2)\n",
    "    return velocity\n",
    "\n",
    "def calculate_side_velocity_stats(velocity_sequence, side_keypoints):\n",
    "    \"\"\"Calculate velocity statistics for a body side\"\"\"\n",
    "    # velocity_sequence shape: (seq_length-1, 33, 2)\n",
    "    side_velocity = velocity_sequence[:, side_keypoints, :]  # (seq_length-1, n_keypoints, 2)\n",
    "    \n",
    "    # Calculate magnitude of velocity vectors\n",
    "    velocity_magnitudes = np.sqrt(np.sum(side_velocity**2, axis=2))  # (seq_length-1, n_keypoints)\n",
    "    \n",
    "    # Aggregate statistics\n",
    "    mean_velocity = np.mean(velocity_magnitudes)\n",
    "    std_velocity = np.std(velocity_magnitudes)\n",
    "    max_velocity = np.max(velocity_magnitudes)\n",
    "    \n",
    "    return mean_velocity, std_velocity, max_velocity\n",
    "\n",
    "def calculate_acceleration(velocity_sequence):\n",
    "    \"\"\"Calculate acceleration from velocity sequence\"\"\"\n",
    "    # velocity_sequence shape: (seq_length-1, 33, 2)\n",
    "    acceleration = np.diff(velocity_sequence, axis=0)  # (seq_length-2, 33, 2)\n",
    "    return acceleration\n",
    "\n",
    "def calculate_side_acceleration_stats(acceleration_sequence, side_keypoints):\n",
    "    \"\"\"Calculate acceleration statistics for a body side\"\"\"\n",
    "    # acceleration_sequence shape: (seq_length-2, 33, 2)\n",
    "    side_acceleration = acceleration_sequence[:, side_keypoints, :]  # (seq_length-2, n_keypoints, 2)\n",
    "    \n",
    "    # Calculate magnitude of acceleration vectors\n",
    "    acceleration_magnitudes = np.sqrt(np.sum(side_acceleration**2, axis=2))  # (seq_length-2, n_keypoints)\n",
    "    \n",
    "    # Aggregate statistics\n",
    "    mean_acceleration = np.mean(acceleration_magnitudes)\n",
    "    std_acceleration = np.std(acceleration_magnitudes)\n",
    "    max_acceleration = np.max(acceleration_magnitudes)\n",
    "    \n",
    "    return mean_acceleration, std_acceleration, max_acceleration\n",
    "\n",
    "def calculate_asymmetry_features(left_stats, right_stats):\n",
    "    \"\"\"Calculate asymmetry ratios between left and right sides\"\"\"\n",
    "    left_mean_vel, left_std_vel, left_max_vel = left_stats['velocity']\n",
    "    right_mean_vel, right_std_vel, right_max_vel = right_stats['velocity']\n",
    "    \n",
    "    left_mean_acc, left_std_acc, left_max_acc = left_stats['acceleration']\n",
    "    right_mean_acc, right_std_acc, right_max_acc = right_stats['acceleration']\n",
    "    \n",
    "    # Velocity asymmetry ratios\n",
    "    vel_mean_ratio = left_mean_vel / (right_mean_vel + 1e-8)  # Avoid division by zero\n",
    "    vel_std_ratio = left_std_vel / (right_std_vel + 1e-8)\n",
    "    vel_max_ratio = left_max_vel / (right_max_vel + 1e-8)\n",
    "    \n",
    "    # Acceleration asymmetry ratios\n",
    "    acc_mean_ratio = left_mean_acc / (right_mean_acc + 1e-8)\n",
    "    acc_std_ratio = left_std_acc / (right_std_acc + 1e-8)\n",
    "    acc_max_ratio = left_max_acc / (right_max_acc + 1e-8)\n",
    "    \n",
    "    return [vel_mean_ratio, vel_std_ratio, vel_max_ratio, \n",
    "            acc_mean_ratio, acc_std_ratio, acc_max_ratio]\n",
    "\n",
    "def extract_dynamic_features(df):\n",
    "    dynamic_features_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        skeleton_seq = np.array(row['Skeleton_Sequence'])\n",
    "\n",
    "\n",
    "        skeleton_seq = skeleton_seq.reshape(skeleton_seq.shape[0], 33, 2)\n",
    "\n",
    "        # Calculate velocity and acceleration\n",
    "        velocity = calculate_velocity(skeleton_seq)\n",
    "        acceleration = calculate_acceleration(velocity)\n",
    "        \n",
    "        # Calculate statistics for each side\n",
    "        left_vel_stats = calculate_side_velocity_stats(velocity, LEFT_SIDE_KEYPOINTS)\n",
    "        right_vel_stats = calculate_side_velocity_stats(velocity, RIGHT_SIDE_KEYPOINTS)\n",
    "        \n",
    "        left_acc_stats = calculate_side_acceleration_stats(acceleration, LEFT_SIDE_KEYPOINTS)\n",
    "        right_acc_stats = calculate_side_acceleration_stats(acceleration, RIGHT_SIDE_KEYPOINTS)\n",
    "        \n",
    "        # Create feature dictionaries\n",
    "        left_stats = {'velocity': left_vel_stats, 'acceleration': left_acc_stats}\n",
    "        right_stats = {'velocity': right_vel_stats, 'acceleration': right_acc_stats}\n",
    "        \n",
    "        # Calculate asymmetry features\n",
    "        asymmetry_features = calculate_asymmetry_features(left_stats, right_stats)\n",
    "        \n",
    "        # Combine all dynamic features\n",
    "        dynamic_features = [\n",
    "            *left_vel_stats, *right_vel_stats,\n",
    "            *left_acc_stats, *right_acc_stats,\n",
    "            *asymmetry_features\n",
    "        ]\n",
    "        \n",
    "        dynamic_features_list.append(dynamic_features)\n",
    "    \n",
    "    return np.array(dynamic_features_list)\n",
    "\n",
    "# Helper function to extract dynamic features for a single sequence\n",
    "def extract_dynamic_features_single(skeleton_seq):\n",
    "    \"\"\"Extract dynamic features for a single skeleton sequence\"\"\"\n",
    "    # Calculate velocity and acceleration\n",
    "    velocity = calculate_velocity(skeleton_seq)\n",
    "    acceleration = calculate_acceleration(velocity)\n",
    "    \n",
    "    # Calculate statistics for each side\n",
    "    left_vel_stats = calculate_side_velocity_stats(velocity, LEFT_SIDE_KEYPOINTS)\n",
    "    right_vel_stats = calculate_side_velocity_stats(velocity, RIGHT_SIDE_KEYPOINTS)\n",
    "    \n",
    "    left_acc_stats = calculate_side_acceleration_stats(acceleration, LEFT_SIDE_KEYPOINTS)\n",
    "    right_acc_stats = calculate_side_acceleration_stats(acceleration, RIGHT_SIDE_KEYPOINTS)\n",
    "    \n",
    "    # Create feature dictionaries\n",
    "    left_stats = {\n",
    "        'velocity': left_vel_stats,\n",
    "        'acceleration': left_acc_stats\n",
    "    }\n",
    "    right_stats = {\n",
    "        'velocity': right_vel_stats,\n",
    "        'acceleration': right_acc_stats\n",
    "    }\n",
    "    \n",
    "    # Calculate asymmetry features\n",
    "    asymmetry_features = calculate_asymmetry_features(left_stats, right_stats)\n",
    "    \n",
    "    # Combine all dynamic features\n",
    "    dynamic_features = [\n",
    "        *left_vel_stats, *right_vel_stats,    # 6 velocity features\n",
    "        *left_acc_stats, *right_acc_stats,    # 6 acceleration features  \n",
    "        *asymmetry_features                   # 6 asymmetry ratios\n",
    "    ]\n",
    "    \n",
    "    return np.array(dynamic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98e6115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined feature extraction function with top-N masking for BOTH static and dynamic features\n",
    "def extract_combined_features_with_masking(df, top_n_mask):\n",
    "    features_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        skeleton_seq = row['Skeleton_Sequence']\n",
    "        exercise_id = row['Exercise_Id']\n",
    "        \n",
    "        # Get feature mask configuration for this exercise\n",
    "        feature_mask_config = top_n_mask.get(exercise_id, {})\n",
    "        \n",
    "        # ===== EXTRACT STATIC FEATURES WITH MASKING =====\n",
    "        # Ensure skeleton_seq is 3D: (seq_length, 33, 2)\n",
    "        if skeleton_seq.ndim == 2 and skeleton_seq.shape[1] == 66:\n",
    "            skeleton_seq = skeleton_seq.reshape(skeleton_seq.shape[0], 33, 2)\n",
    "        \n",
    "        # Calculate means and variances for all keypoints first\n",
    "        flattened = skeleton_seq.reshape(len(skeleton_seq), -1)  # (seq_length, 66)\n",
    "        all_means = np.mean(flattened, axis=0)  # 66 features\n",
    "        all_variances = np.var(flattened, axis=0)  # 66 features\n",
    "        \n",
    "        # Apply granular masking - only keep specified static features\n",
    "        final_means = np.zeros(66)\n",
    "        final_variances = np.zeros(66)\n",
    "        \n",
    "        # For each keypoint in the mask configuration, keep specified components\n",
    "        for kp_idx, components_to_keep in feature_mask_config.items():\n",
    "            if kp_idx == 'dynamic':\n",
    "                continue  # Skip dynamic features for now\n",
    "            \n",
    "            # Each keypoint has 2 positions in the mean/variance arrays\n",
    "            mean_x_idx = kp_idx * 2\n",
    "            mean_y_idx = kp_idx * 2 + 1\n",
    "            var_x_idx = kp_idx * 2\n",
    "            var_y_idx = kp_idx * 2 + 1\n",
    "            \n",
    "            if 'mean_x' in components_to_keep:\n",
    "                final_means[mean_x_idx] = all_means[mean_x_idx]\n",
    "            if 'mean_y' in components_to_keep:\n",
    "                final_means[mean_y_idx] = all_means[mean_y_idx]\n",
    "            if 'var_x' in components_to_keep:\n",
    "                final_variances[var_x_idx] = all_variances[var_x_idx]\n",
    "            if 'var_y' in components_to_keep:\n",
    "                final_variances[var_y_idx] = all_variances[var_y_idx]\n",
    "        \n",
    "        static_features = np.concatenate([final_means, final_variances])\n",
    "        \n",
    "        # ===== EXTRACT DYNAMIC FEATURES WITH MASKING =====\n",
    "        dynamic_features_all = extract_dynamic_features_single(skeleton_seq)\n",
    "        \n",
    "        # Apply masking to dynamic features\n",
    "        dynamic_features_masked = np.zeros(len(dynamic_feature_names))\n",
    "        if 'dynamic' in feature_mask_config:\n",
    "            dynamic_indices_to_keep = feature_mask_config['dynamic']\n",
    "            for idx in dynamic_indices_to_keep:\n",
    "                if idx < len(dynamic_features_all):\n",
    "                    dynamic_features_masked[idx] = dynamic_features_all[idx]\n",
    "        \n",
    "        # ===== COMBINE STATIC AND DYNAMIC FEATURES =====\n",
    "        combined_features = np.concatenate([static_features, dynamic_features_masked])\n",
    "        features_list.append(combined_features)\n",
    "    \n",
    "    return np.array(features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b21163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create feature mask for top N features per exercise (BOTH static and dynamic)\n",
    "def create_top_n_mask(n_features_per_exercise):\n",
    "    mask_dict = {}\n",
    "    \n",
    "    for exercise in ['E1', 'E2', 'E3', 'E4', 'E5']:\n",
    "        # Get top N features for this exercise (both static and dynamic)\n",
    "        top_features = feature_importance_df[\n",
    "            feature_importance_df['exercise'] == exercise\n",
    "        ].nlargest(n_features_per_exercise, 'importance')\n",
    "        \n",
    "        mask_dict[exercise] = {}\n",
    "        \n",
    "        for _, row in top_features.iterrows():\n",
    "            feature_name = row['feature']\n",
    "            if feature_name in feature_to_index:\n",
    "                feature_idx, feature_type = feature_to_index[feature_name]\n",
    "                \n",
    "                if feature_type == 'dynamic':\n",
    "                    # For dynamic features, we store them with a special key\n",
    "                    if 'dynamic' not in mask_dict[exercise]:\n",
    "                        mask_dict[exercise]['dynamic'] = set()\n",
    "                    mask_dict[exercise]['dynamic'].add(feature_idx - 132)  # Convert to dynamic feature index (0-17)\n",
    "                else:\n",
    "                    # For static features, use the original keypoint-based system\n",
    "                    kp_idx = feature_idx\n",
    "                    component = feature_type\n",
    "                    if kp_idx not in mask_dict[exercise]:\n",
    "                        mask_dict[exercise][kp_idx] = []\n",
    "                    if component not in mask_dict[exercise][kp_idx]:\n",
    "                        mask_dict[exercise][kp_idx].append(component)\n",
    "    \n",
    "    return mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f81c7477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_cv(model, X_combined, y_sequences, sequence_patient_ids, \n",
    "                 patient_to_label, patient_ids, Y_train_filtered, n_splits=100):\n",
    "    \"\"\"Run random leave-3-out cross-validation with n_splits random combinations\"\"\"\n",
    "    majority_accuracies = []\n",
    "    probability_accuracies = []\n",
    "    \n",
    "    # Generate all possible test combinations\n",
    "    all_test_combinations = list(combinations(patient_ids, 3))\n",
    "    \n",
    "    # Randomly select n_splits combinations\n",
    "    rng = np.random.RandomState(42)\n",
    "    selected_combinations = rng.choice(len(all_test_combinations), \n",
    "                                     size=min(n_splits, len(all_test_combinations)), \n",
    "                                     replace=False)\n",
    "    \n",
    "    for idx in tqdm(selected_combinations, desc=f\"Random CV ({n_splits} splits)\", leave=False):\n",
    "        test_patients_tuple = all_test_combinations[idx]\n",
    "        test_patients = np.array(test_patients_tuple)\n",
    "        train_patients = np.setdiff1d(patient_ids, test_patients)\n",
    "        \n",
    "        # Create masks\n",
    "        train_mask = np.isin(sequence_patient_ids, train_patients)\n",
    "        test_mask = np.isin(sequence_patient_ids, test_patients)\n",
    "        \n",
    "        # Split data\n",
    "        X_train_split = X_combined[train_mask]\n",
    "        X_test_split = X_combined[test_mask]\n",
    "        y_train_split = y_sequences[train_mask]\n",
    "        test_patient_ids_split = sequence_patient_ids[test_mask]\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_split, y_train_split)\n",
    "        \n",
    "        # Get predictions\n",
    "        test_sequence_preds = model.predict(X_test_split)\n",
    "        test_sequence_probs = model.predict_proba(X_test_split)\n",
    "        \n",
    "        # Aggregate to patient level\n",
    "        majority_acc, probability_acc = aggregate_patient_predictions(\n",
    "            test_sequence_preds, test_sequence_probs, test_patient_ids_split, \n",
    "            test_patients, patient_to_label\n",
    "        )\n",
    "        \n",
    "        majority_accuracies.append(majority_acc)\n",
    "        probability_accuracies.append(probability_acc)\n",
    "    \n",
    "    return majority_accuracies, probability_accuracies\n",
    "\n",
    "def aggregate_patient_predictions(test_sequence_preds, test_sequence_probs, \n",
    "                                 test_patient_ids_split, test_patients, patient_to_label):\n",
    "    \"\"\"Aggregate sequence predictions to patient level using both methods\"\"\"\n",
    "    # Majority Voting\n",
    "    patient_predictions = {}\n",
    "    for i, patient_id in enumerate(test_patient_ids_split):\n",
    "        if patient_id not in patient_predictions:\n",
    "            patient_predictions[patient_id] = []\n",
    "        patient_predictions[patient_id].append(test_sequence_preds[i])\n",
    "    \n",
    "    majority_preds = []\n",
    "    true_labels = []\n",
    "    for patient_id in test_patients:\n",
    "        votes = patient_predictions[patient_id]\n",
    "        pred_label = np.argmax(np.bincount(votes))\n",
    "        majority_preds.append(pred_label)\n",
    "        true_labels.append(patient_to_label[patient_id])\n",
    "    \n",
    "    majority_acc = balanced_accuracy_score(true_labels, majority_preds)\n",
    "    \n",
    "    # Probability Averaging\n",
    "    patient_probs = {}\n",
    "    for i, patient_id in enumerate(test_patient_ids_split):\n",
    "        if patient_id not in patient_probs:\n",
    "            patient_probs[patient_id] = []\n",
    "        patient_probs[patient_id].append(test_sequence_probs[i])\n",
    "    \n",
    "    probability_preds = []\n",
    "    for patient_id in test_patients:\n",
    "        avg_probs = np.mean(patient_probs[patient_id], axis=0)\n",
    "        pred_label = np.argmax(avg_probs)\n",
    "        probability_preds.append(pred_label)\n",
    "    \n",
    "    probability_acc = balanced_accuracy_score(true_labels, probability_preds)\n",
    "    \n",
    "    return majority_acc, probability_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf_hyperparameter_search():\n",
    "    # Configuration\n",
    "    TOP_FEATURES = 18\n",
    "    N_SPLITS = 50  # Use 50 random splits instead of exhaustive\n",
    "    print(f\"Running Random Forest hyperparameter optimization with top {TOP_FEATURES} features...\")\n",
    "    print(f\"Using {N_SPLITS} random splits for faster evaluation\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    X_train = joblib.load(\"Data/Xtrain2.pkl\")\n",
    "    Y_train = np.load('Data/Ytrain2.npy')\n",
    "    \n",
    "    # Get patient IDs and create mappings\n",
    "    patient_ids = np.sort(X_train['Patient_Id'].unique())\n",
    "    all_patient_to_label = dict(zip(range(1, 15), Y_train))\n",
    "    patient_to_label = {pid: all_patient_to_label[pid] for pid in patient_ids}\n",
    "    Y_train_filtered = np.array([patient_to_label[pid] for pid in patient_ids])\n",
    "    \n",
    "    print(f\"Data loaded: {X_train.shape[0]} sequences, {len(patient_ids)} patients\")\n",
    "    \n",
    "    # Load feature importance and create mask for top N features\n",
    "    print(\"Loading feature importance and creating feature mask...\")\n",
    "    feature_importance_df = pd.read_csv('feature_importance_v2_baseline.csv')\n",
    "    top_n_mask = create_top_n_mask(TOP_FEATURES)\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"Extracting features...\")\n",
    "    X_combined_features = extract_combined_features_with_masking(X_train, top_n_mask)\n",
    "    \n",
    "    # Add exercise encoding\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    exercise_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    exercise_encoded = exercise_encoder.fit_transform(X_train[['Exercise_Id']])\n",
    "    X_combined = np.concatenate([X_combined_features, exercise_encoded], axis=1)\n",
    "    \n",
    "    # Create sequence-level labels and patient IDs\n",
    "    y_sequences = X_train['Patient_Id'].map(patient_to_label).values\n",
    "    sequence_patient_ids = X_train['Patient_Id'].values\n",
    "    \n",
    "    print(f\"Final feature matrix shape: {X_combined.shape}\")\n",
    "    \n",
    "    # Define SMALLER Random Forest hyperparameter grid\n",
    "    param_grid = [\n",
    "        {\n",
    "            'rf__n_estimators': [50, 100],  # Reduced from 3 to 2 options\n",
    "            'rf__max_depth': [5, 10, None],  # Reduced from 5 to 3 options\n",
    "            'rf__min_samples_split': [2, 5],  # Reduced from 3 to 2 options\n",
    "            'rf__min_samples_leaf': [1, 2],   # Reduced from 3 to 2 options\n",
    "            'rf__max_features': ['sqrt', 'log2']  # Removed None, kept 2 options\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "    all_params = list(ParameterGrid(param_grid))\n",
    "    print(f\"Testing {len(all_params)} hyperparameter combinations (reduced grid)...\")\n",
    "    \n",
    "    # Results storage\n",
    "    results = []\n",
    "    \n",
    "    # Test each hyperparameter combination\n",
    "    for params in tqdm(all_params, desc=\"Random Forest hyperparameter search\"):\n",
    "        # Create pipeline with current parameters\n",
    "        pipeline = Pipeline([\n",
    "            # Remove scaler for Random Forest (not needed and saves computation)\n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators=params['rf__n_estimators'],\n",
    "                max_depth=params['rf__max_depth'],\n",
    "                min_samples_split=params['rf__min_samples_split'],\n",
    "                min_samples_leaf=params['rf__min_samples_leaf'],\n",
    "                max_features=params['rf__max_features'],\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                n_jobs=-1  # Use all cores\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Run RANDOM cross-validation (100 splits instead of exhaustive)\n",
    "        majority_accuracies, probability_accuracies = run_random_cv(\n",
    "            pipeline, X_combined, y_sequences, sequence_patient_ids, \n",
    "            patient_to_label, patient_ids, Y_train_filtered, n_splits=N_SPLITS\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'n_estimators': params['rf__n_estimators'],\n",
    "            'max_depth': params['rf__max_depth'],\n",
    "            'min_samples_split': params['rf__min_samples_split'],\n",
    "            'min_samples_leaf': params['rf__min_samples_leaf'],\n",
    "            'max_features': params['rf__max_features'],\n",
    "            'majority_mean': np.mean(majority_accuracies),\n",
    "            'majority_std': np.std(majority_accuracies),\n",
    "            'majority_min': np.min(majority_accuracies),\n",
    "            'majority_max': np.max(majority_accuracies),\n",
    "            'probability_mean': np.mean(probability_accuracies),\n",
    "            'probability_std': np.std(probability_accuracies),\n",
    "            'probability_min': np.min(probability_accuracies),\n",
    "            'probability_max': np.max(probability_accuracies)\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    # Convert to DataFrame and find best models\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Find best parameters for each method\n",
    "    best_majority = results_df.loc[results_df['majority_mean'].idxmax()]\n",
    "    best_probability = results_df.loc[results_df['probability_mean'].idxmax()]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RANDOM FOREST HYPERPARAMETER OPTIMIZATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nBEST MAJORITY VOTING MODEL:\")\n",
    "    print(f\"  Parameters: n_estimators={best_majority['n_estimators']}, \"\n",
    "          f\"max_depth={best_majority['max_depth']}, \"\n",
    "          f\"min_samples_split={best_majority['min_samples_split']}, \"\n",
    "          f\"min_samples_leaf={best_majority['min_samples_leaf']}, \"\n",
    "          f\"max_features={best_majority['max_features']}\")\n",
    "    print(f\"  Performance: {best_majority['majority_mean']:.4f} (±{best_majority['majority_std']:.4f})\")\n",
    "    print(f\"  Range: [{best_majority['majority_min']:.4f}, {best_majority['majority_max']:.4f}]\")\n",
    "    \n",
    "    print(f\"\\nBEST PROBABILITY AVERAGING MODEL:\")\n",
    "    print(f\"  Parameters: n_estimators={best_probability['n_estimators']}, \"\n",
    "          f\"max_depth={best_probability['max_depth']}, \"\n",
    "          f\"min_samples_split={best_probability['min_samples_split']}, \"\n",
    "          f\"min_samples_leaf={best_probability['min_samples_leaf']}, \"\n",
    "          f\"max_features={best_probability['max_features']}\")\n",
    "    print(f\"  Performance: {best_probability['probability_mean']:.4f} (±{best_probability['probability_std']:.4f})\")\n",
    "    print(f\"  Range: [{best_probability['probability_min']:.4f}, {best_probability['probability_max']:.4f}]\")\n",
    "    \n",
    "    return results_df, best_majority, best_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f62a7a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Random Forest hyperparameter optimization with top 18 features...\n",
      "Using 50 random splits for faster evaluation\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 444 sequences, 14 patients\n",
      "Loading feature importance and creating feature mask...\n",
      "Extracting features...\n",
      "Final feature matrix shape: (444, 155)\n",
      "Testing 48 hyperparameter combinations (reduced grid)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random Forest hyperparameter search: 100%|██████████| 48/48 [09:16<00:00, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RANDOM FOREST HYPERPARAMETER OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "BEST MAJORITY VOTING MODEL:\n",
      "  Parameters: n_estimators=100, max_depth=5.0, min_samples_split=2, min_samples_leaf=2, max_features=sqrt\n",
      "  Performance: 0.7600 (±0.2481)\n",
      "  Range: [0.2500, 1.0000]\n",
      "\n",
      "BEST PROBABILITY AVERAGING MODEL:\n",
      "  Parameters: n_estimators=100, max_depth=5.0, min_samples_split=2, min_samples_leaf=2, max_features=sqrt\n",
      "  Performance: 0.7667 (±0.2500)\n",
      "  Range: [0.2500, 1.0000]\n",
      "\n",
      "Detailed results saved to: rf_hyperparameter_results_top18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the optimization\n",
    "if __name__ == \"__main__\":\n",
    "    results_df, best_majority, best_probability = run_rf_hyperparameter_search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
